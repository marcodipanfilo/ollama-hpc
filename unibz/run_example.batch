#!/bin/bash
#SBATCH --job-name=ollama-test
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=02:00:00
#SBATCH --gres=gpu:1      # if you want GPU acceleration
#SBATCH --constraint=gpu32g # liste von gpus quality siehe hpc documentation hidden lanz
#SBATCH --account=vkg_mpboot
#SBATCH --partition=gpu-pre
#SBATCH --output=logs/%j.out # the standard output is written to this file
#SBATCH --error=logs/%j.err # the standard error is written to this file

# module load ollama   # if you have a module system, otherwise ensure Ollama is installed

model load cuda
export PATH=$HOME/.local/bin:$PATH

# removes debug info
export GIN_MODE=release
export OLLAMA_HOST=0.0.0.0
export OLLAMA_MODELS=/data/users/mdipanfilo/ollama

# Start the Ollama server in the background
ollama serve >/dev/null 2>&1 &

# Wait a bit to ensure the server is ready
sleep 5

ollama pull qwen3-vl:8b
ollama pull qwen3:8b
ollama run deepseek-r1:8b "What is the capital of Italy?"
ollama list
